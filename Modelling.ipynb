{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T20:10:24.474456Z",
     "start_time": "2019-07-01T20:10:24.468454Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import re\n",
    "import math\n",
    "import requests as re\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, roc_auc_score, confusion_matrix, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical, np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:25:18.953607Z",
     "start_time": "2019-07-01T11:25:15.064023Z"
    }
   },
   "outputs": [],
   "source": [
    "Data = pd.read_csv(\"CleanData.csv\").drop(columns = [\"Unnamed: 0\",\"LyricsOriginal\"])\n",
    "Data[\"TrackArtist\"] = Data.Track +\" By \"+ Data.Artist1\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artists Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T10:10:33.549999Z",
     "start_time": "2019-07-01T10:10:33.408098Z"
    }
   },
   "outputs": [],
   "source": [
    "ArtistAll = Data.Artist2[Data.Artist2!=\"Nan\"].value_counts()[0:10]\n",
    "ArtistTop10 = Data.Artist2[(Data.Artist2!=\"Nan\") & (Data.Rank <= 10)].value_counts()[0:10]\n",
    "ArtistTop3 = Data.Artist2[(Data.Artist2!=\"Nan\") & (Data.Rank <= 3)].value_counts()[0:10]\n",
    "ArtistTop1 = Data.Artist2[(Data.Artist2!=\"Nan\") & (Data.Rank <= 1)].value_counts()[0:10]\n",
    "ArtistData = pd.DataFrame({\n",
    "    \"Rank\" : list(range(1,11)),\n",
    "    \"MostWeeksWithSongsAtTopCharts\" : [str(i)+\" : \"+str(j) for j, i in zip(ArtistTop1.tolist(),pd.DataFrame(ArtistTop1).index.tolist())],\n",
    "    \"MostWeeksWithSongsInTop3Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(ArtistTop3.tolist(),pd.DataFrame(ArtistTop3).index.tolist())],\n",
    "    \"MostWeeksWithSongsInTop10Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(ArtistTop10.tolist(),pd.DataFrame(ArtistTop10).index.tolist())],\n",
    "    \"MostWeeksWithSongsInTop100Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(ArtistAll.tolist(),pd.DataFrame(ArtistAll).index.tolist())]\n",
    "})\n",
    "ArtistData.to_csv(\"Leaderboards/ArtistLeaderboards.csv\")\n",
    "ArtistData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Songs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-01T10:09:54.686Z"
    }
   },
   "outputs": [],
   "source": [
    "SongsAll = Data.TrackArtist[Data.TrackArtist!=\"Nan\"].value_counts()[0:10]\n",
    "SongsTop10 = Data.TrackArtist[(Data.TrackArtist!=\"Nan\") & (Data.Rank <= 10)].value_counts()[0:10]\n",
    "SongsTop3 = Data.TrackArtist[(Data.TrackArtist!=\"Nan\") & (Data.Rank <= 3)].value_counts()[0:10]\n",
    "SongsTop1 = Data.TrackArtist[(Data.TrackArtist!=\"Nan\") & (Data.Rank <= 1)].value_counts()[0:10]\n",
    "SongsData = pd.DataFrame({\n",
    "    \"Rank\" : list(range(1,11)),\n",
    "    \"MostWeeksAtTopCharts\" : [str(i)+\" : \"+str(j) for j, i in zip(SongsTop1.tolist(),pd.DataFrame(SongsTop1).index.tolist())],\n",
    "    \"MostWeeksInTop3Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(SongsTop3.tolist(),pd.DataFrame(SongsTop3).index.tolist())],\n",
    "    \"MostWeeksInTop10Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(SongsTop10.tolist(),pd.DataFrame(SongsTop10).index.tolist())],\n",
    "    \"MostWeeksInTop100Charts\" : [str(i)+\" : \"+str(j) for j, i in zip(SongsAll.tolist(),pd.DataFrame(SongsAll).index.tolist())]\n",
    "})\n",
    "SongsData.to_csv(\"Leaderboards/SongLeaderboards.csv\")\n",
    "SongsData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-01T10:09:54.689Z"
    }
   },
   "outputs": [],
   "source": [
    "GenreData = pd.DataFrame({\"Date\" : Data.Date.unique()})\n",
    "GenreData[\"Blues\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Blues\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Rock\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Rock\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Pop\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Pop\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"HipHop\"] = [Data[(Data.Date == i) & (Data.Genre2==\"HipHop\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"R&B\"] = [Data[(Data.Date == i) & (Data.Genre2==\"R&B\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Country\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Country\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Folk\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Folk\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Reggae\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Reggae\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Electro\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Electro\")].shape[0] for i in GenreData.Date]\n",
    "GenreData[\"Religous\"] = [Data[(Data.Date == i) & (Data.Genre2==\"Religous\")].shape[0] for i in GenreData.Date]\n",
    "GenreData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genre Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-01T10:09:54.692Z"
    }
   },
   "outputs": [],
   "source": [
    "GenreData.to_csv(\"GenreData.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Unique Song Dataset For Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:36:36.791052Z",
     "start_time": "2019-07-01T11:25:33.971106Z"
    }
   },
   "outputs": [],
   "source": [
    "UniqueData = Data[[\"TrackArtist\",\"Artist2\",\"Genre1\",\"Genre2\",\"LyricsClean\"]].drop_duplicates()\n",
    "UniqueData = UniqueData[UniqueData.TrackArtist.isna() == False]\n",
    "UniqueData[\"MaxRank\"] = [max(Data.Rank[Data.TrackArtist == i]) for i in UniqueData.TrackArtist]\n",
    "UniqueData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:36:47.720093Z",
     "start_time": "2019-07-01T11:36:47.544391Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(UniqueData.MaxRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:37:03.598931Z",
     "start_time": "2019-07-01T11:37:01.209215Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in UniqueData.Genre2.unique():\n",
    "    if pd.notna(i):\n",
    "        fig, axs = plt.subplots(ncols=1, figsize=(15,5))\n",
    "        sns.distplot(UniqueData.MaxRank[UniqueData.Genre2 == i], ax = axs).set_title(\"Highest Rank Achieved - {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training, Test, X & Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:37:10.762216Z",
     "start_time": "2019-07-01T11:37:10.749877Z"
    }
   },
   "outputs": [],
   "source": [
    "UniqueData = UniqueData.dropna(subset = [\"LyricsClean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T11:37:38.131589Z",
     "start_time": "2019-07-01T11:37:37.476921Z"
    }
   },
   "outputs": [],
   "source": [
    "UniqueData.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:02.783100Z",
     "start_time": "2019-07-01T23:15:02.489605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrackArtist</th>\n",
       "      <th>Artist2</th>\n",
       "      <th>Genre1</th>\n",
       "      <th>Genre2</th>\n",
       "      <th>LyricsClean</th>\n",
       "      <th>MaxRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poor Little Fool By Ricky Nelson</td>\n",
       "      <td>Ricky Nelson</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>I used to play around with hearts that hastene...</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Patricia By Perez Prado And His Orchestra</td>\n",
       "      <td>Perez Prado</td>\n",
       "      <td>Latin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Splish Splash By  Bobby Darin</td>\n",
       "      <td>Bobby Darin</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>Blues</td>\n",
       "      <td>splish splash I was taking a bath long about a...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hard Headed Woman By Elvis Presley With The Jo...</td>\n",
       "      <td>Elvis Presley</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Well a hard headed woman, A soft hearted man B...</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rebel-'rouser By Duane Eddy His Twangy Guitar ...</td>\n",
       "      <td>Duane Eddy</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Instrumental</td>\n",
       "      <td>52.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         TrackArtist         Artist2  \\\n",
       "0                   Poor Little Fool By Ricky Nelson    Ricky Nelson   \n",
       "1          Patricia By Perez Prado And His Orchestra     Perez Prado   \n",
       "2                      Splish Splash By  Bobby Darin     Bobby Darin   \n",
       "3  Hard Headed Woman By Elvis Presley With The Jo...  Elvis Presley    \n",
       "4  Rebel-'rouser By Duane Eddy His Twangy Guitar ...     Duane Eddy    \n",
       "\n",
       "     Genre1 Genre2                                        LyricsClean  MaxRank  \n",
       "0  Pop/Rock   Rock  I used to play around with hearts that hastene...     71.0  \n",
       "1     Latin    NaN                                       Instrumental     85.0  \n",
       "2  Pop/Rock  Blues  splish splash I was taking a bath long about a...      3.0  \n",
       "3  Pop/Rock   Rock  Well a hard headed woman, A soft hearted man B...     94.0  \n",
       "4  Pop/Rock   Rock                                       Instrumental     52.0  "
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniqueData = pd.read_csv(\"temp.csv\").drop(columns = \"Unnamed: 0\")\n",
    "UniqueData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:02.895012Z",
     "start_time": "2019-07-01T23:15:02.882959Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numebr Of Nas For Genre1 343\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pop/Rock         10844\n",
       "R&B               5073\n",
       "Country           2184\n",
       "Rap               1918\n",
       "Vocal              829\n",
       "Electronic         494\n",
       "Jazz               227\n",
       "Latin              198\n",
       "Blues              180\n",
       "Folk               171\n",
       "Easy               144\n",
       "Reggae             129\n",
       "Stage               75\n",
       "Religious           61\n",
       "Comedy/Spoken       39\n",
       "International       27\n",
       "Children            18\n",
       "Classical           12\n",
       "New                  8\n",
       "Avant                4\n",
       "Holiday              1\n",
       "Name: Genre1, dtype: int64"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Numebr Of Nas For Genre1\",len(UniqueData.Genre1)-UniqueData.Genre1.count())\n",
    "UniqueData.Genre1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:03.604808Z",
     "start_time": "2019-07-01T23:15:03.593591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numebr Of Nas For Genre2 4039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Rock        5239\n",
       "Pop         4090\n",
       "R&B         3011\n",
       "Country     2221\n",
       "HipHop      1871\n",
       "Blues       1678\n",
       "Folk         339\n",
       "Electro      249\n",
       "Reggae       168\n",
       "Religous      74\n",
       "Name: Genre2, dtype: int64"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Numebr Of Nas For Genre2\",len(UniqueData.Genre2)-UniqueData.Genre2.count())\n",
    "UniqueData.Genre2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:06.190028Z",
     "start_time": "2019-07-01T23:15:04.473337Z"
    }
   },
   "outputs": [],
   "source": [
    "UniqueData = UniqueData.dropna(subset=[\"Genre1\"])\n",
    "UniqueData = UniqueData[UniqueData.Genre1.isin([\"Pop/Rock\"\n",
    "                                               ,\"R&B\"       \n",
    "                                               ,\"Country\"   \n",
    "                                               ,\"Rap\"       \n",
    "                                               ,\"Vocal\"     \n",
    "                                               ,\"Electronic\"\n",
    "                                               ,\"Jazz\"      \n",
    "                                               ,\"Latin\"     \n",
    "                                               ,\"Blues\"     \n",
    "                                               ,\"Folk\"      \n",
    "                                               ,\"Easy\"      \n",
    "                                               ,\"Reggae\"])]\n",
    "UniqueData.loc[UniqueData.Genre1==\"Jazz\"] = \"Blues\"\n",
    "UniqueData.loc[UniqueData.Genre1==\"Vocal\"] = \"Folk\"\n",
    "XTrain, XTest, YTrain, YTest = train_test_split(UniqueData[\"LyricsClean\"], UniqueData['Genre1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:16.332930Z",
     "start_time": "2019-07-01T23:15:09.121872Z"
    }
   },
   "outputs": [],
   "source": [
    "Vectorizer = TfidfVectorizer()\n",
    "Vectorizer.fit(XTrain)\n",
    "XTrain = Vectorizer.transform(XTrain)\n",
    "XTest  = Vectorizer.transform(XTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:21.130661Z",
     "start_time": "2019-07-01T23:15:16.407648Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=-1,\n",
       "          penalty='l2', random_state=None, solver='warn', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Classifier = LogisticRegression(n_jobs=-1)\n",
    "Classifier.fit(XTrain, YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:21.211809Z",
     "start_time": "2019-07-01T23:15:21.206068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Pop/Rock', 'Latin', 'R&B', 'Country', 'Blues', 'Folk', 'Easy',\n",
       "       'Reggae', 'Electronic', 'Rap'], dtype=object)"
      ]
     },
     "execution_count": 717,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UniqueData.Genre1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:22.994015Z",
     "start_time": "2019-07-01T23:15:22.854033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6823081045673792\n",
      "0.6336191496963202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  60,    0,    0,    0,    0,    0,   44,    6,    1,    0],\n",
       "       [   0,   43,    0,    0,    0,    0,  431,   51,    4,    0],\n",
       "       [   0,    0,    1,    0,    0,    0,   29,    5,    0,    0],\n",
       "       [   0,    3,    0,    1,    0,    0,  104,   16,    4,    0],\n",
       "       [   0,    0,    0,    0,  213,    0,   45,    9,    0,    0],\n",
       "       [   0,    0,    0,    0,    0,    9,   35,    8,    0,    0],\n",
       "       [   0,   10,    0,    0,    0,    3, 2500,  184,   14,    0],\n",
       "       [   1,    5,    0,    0,    0,    0,  755,  443,   37,    0],\n",
       "       [   0,    2,    0,    0,    0,    0,  185,   29,  273,    0],\n",
       "       [   0,    0,    0,    0,    0,    0,   23,    4,    4,    4]])"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(YTrain,Classifier.predict(XTrain), average=\"micro\"))\n",
    "print(f1_score(YTest,Classifier.predict(XTest), average=\"micro\"))\n",
    "confusion_matrix(YTest,Classifier.predict(XTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Deep Learning Model - Sequential Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:15:48.859865Z",
     "start_time": "2019-07-01T23:15:48.850415Z"
    }
   },
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Encoder.fit(YTrain)\n",
    "YTrainNum = Encoder.transform(YTrain)\n",
    "YTrainDummy = np_utils.to_categorical(YTrainNum)\n",
    "YTestNum = Encoder.transform(YTest)\n",
    "YTestDummy = np_utils.to_categorical(YTestNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:16:02.510474Z",
     "start_time": "2019-07-01T23:16:02.394005Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_239 (Dense)            (None, 30)                1131330   \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_241 (Dense)            (None, 10)                210       \n",
      "=================================================================\n",
      "Total params: 1,132,160\n",
      "Trainable params: 1,132,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "InputDim = XTrain.shape[1]\n",
    "Model = Sequential()\n",
    "Model.add(layers.Dense(30, input_dim=InputDim, activation='relu'))\n",
    "Model.add(layers.Dropout(rate = 0.4))\n",
    "Model.add(layers.Dense(20, activation='relu'))\n",
    "Model.add(layers.Dropout(rate = 0.4))\n",
    "Model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "Model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:20:16.301538Z",
     "start_time": "2019-07-01T23:16:04.333507Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16793 samples, validate on 5598 samples\n",
      "Epoch 1/20\n",
      "16793/16793 [==============================] - 21s 1ms/step - loss: 1.5458 - acc: 0.4844 - val_loss: 1.2748 - val_acc: 0.5632\n",
      "Epoch 2/20\n",
      "16793/16793 [==============================] - 12s 724us/step - loss: 1.2428 - acc: 0.5710 - val_loss: 1.1472 - val_acc: 0.6068\n",
      "Epoch 3/20\n",
      "16793/16793 [==============================] - 12s 726us/step - loss: 1.0784 - acc: 0.6290 - val_loss: 1.0847 - val_acc: 0.6518\n",
      "Epoch 4/20\n",
      "16793/16793 [==============================] - 12s 720us/step - loss: 0.9462 - acc: 0.6789 - val_loss: 1.0667 - val_acc: 0.6606\n",
      "Epoch 5/20\n",
      "16793/16793 [==============================] - 12s 720us/step - loss: 0.8441 - acc: 0.7174 - val_loss: 1.0737 - val_acc: 0.6681\n",
      "Epoch 6/20\n",
      "16793/16793 [==============================] - 12s 724us/step - loss: 0.7628 - acc: 0.7461 - val_loss: 1.0818 - val_acc: 0.6740\n",
      "Epoch 7/20\n",
      "16793/16793 [==============================] - 12s 723us/step - loss: 0.7038 - acc: 0.7640 - val_loss: 1.1177 - val_acc: 0.6802\n",
      "Epoch 8/20\n",
      "16793/16793 [==============================] - 12s 726us/step - loss: 0.6543 - acc: 0.7813 - val_loss: 1.1519 - val_acc: 0.6911\n",
      "Epoch 9/20\n",
      "16793/16793 [==============================] - 12s 728us/step - loss: 0.6158 - acc: 0.7939 - val_loss: 1.1887 - val_acc: 0.6906\n",
      "Epoch 10/20\n",
      "16793/16793 [==============================] - 12s 727us/step - loss: 0.5800 - acc: 0.8057 - val_loss: 1.2385 - val_acc: 0.6919\n",
      "Epoch 11/20\n",
      "16793/16793 [==============================] - 12s 724us/step - loss: 0.5572 - acc: 0.8103 - val_loss: 1.2508 - val_acc: 0.6927\n",
      "Epoch 12/20\n",
      "16793/16793 [==============================] - 12s 725us/step - loss: 0.5381 - acc: 0.8200 - val_loss: 1.3444 - val_acc: 0.6922\n",
      "Epoch 13/20\n",
      "16793/16793 [==============================] - 12s 720us/step - loss: 0.5200 - acc: 0.8222 - val_loss: 1.3457 - val_acc: 0.6947\n",
      "Epoch 14/20\n",
      "16793/16793 [==============================] - 12s 722us/step - loss: 0.4944 - acc: 0.8345 - val_loss: 1.3801 - val_acc: 0.6969\n",
      "Epoch 15/20\n",
      "16793/16793 [==============================] - 12s 721us/step - loss: 0.4886 - acc: 0.8339 - val_loss: 1.4426 - val_acc: 0.6940\n",
      "Epoch 16/20\n",
      "16793/16793 [==============================] - 12s 720us/step - loss: 0.4686 - acc: 0.8406 - val_loss: 1.4644 - val_acc: 0.6981\n",
      "Epoch 17/20\n",
      "16793/16793 [==============================] - 12s 726us/step - loss: 0.4525 - acc: 0.8437 - val_loss: 1.5517 - val_acc: 0.6965\n",
      "Epoch 18/20\n",
      "16793/16793 [==============================] - 12s 720us/step - loss: 0.4469 - acc: 0.8462 - val_loss: 1.5575 - val_acc: 0.6947\n",
      "Epoch 19/20\n",
      "16793/16793 [==============================] - 12s 725us/step - loss: 0.4410 - acc: 0.8501 - val_loss: 1.6046 - val_acc: 0.6904\n",
      "Epoch 20/20\n",
      "16793/16793 [==============================] - 12s 721us/step - loss: 0.4339 - acc: 0.8538 - val_loss: 1.6149 - val_acc: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1268f8f5f8>"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(XTrain, YTrainDummy,\n",
    "          epochs=20,\n",
    "          validation_data=(XTest, YTestDummy),\n",
    "          batch_size=25,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:20:28.522747Z",
     "start_time": "2019-07-01T23:20:24.096550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.905079497409635\n",
      "0.6934619506966774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  64,    3,    0,    4,    0,    0,   34,    6,    0,    0],\n",
       "       [   1,  229,    0,    0,    2,    0,  217,   76,    4,    0],\n",
       "       [   0,    0,    0,   11,    0,    0,   19,    5,    0,    0],\n",
       "       [   1,    9,    0,   36,    0,    0,   56,   23,    3,    0],\n",
       "       [   0,    3,    0,    0,  227,    0,   21,   15,    1,    0],\n",
       "       [   1,    2,    0,    6,    0,   10,   14,   19,    0,    0],\n",
       "       [   5,  107,    0,   18,    2,    5, 2162,  367,   45,    0],\n",
       "       [   1,   34,    0,   10,    2,    0,  363,  800,   31,    0],\n",
       "       [   1,    5,    0,    1,    2,    0,   88,   38,  354,    0],\n",
       "       [   0,    1,    0,   10,    0,    0,   15,    6,    3,    0]])"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f1_score(YTrainNum,Model.predict_classes(XTrain), average=\"micro\"))\n",
    "print(f1_score(YTestNum,Model.predict_classes(XTest), average=\"micro\"))\n",
    "confusion_matrix(YTestNum,Model.predict_classes(XTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:21:26.780853Z",
     "start_time": "2019-07-01T23:21:26.773110Z"
    }
   },
   "outputs": [],
   "source": [
    "XTrain, XTest, YTrain, YTest = train_test_split(UniqueData[\"LyricsClean\"], UniqueData['Genre1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:21:29.125033Z",
     "start_time": "2019-07-01T23:21:29.116161Z"
    }
   },
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Encoder.fit(YTrain)\n",
    "YTrainNum = Encoder.transform(YTrain)\n",
    "YTrainDummy = np_utils.to_categorical(YTrainNum)\n",
    "YTestNum = Encoder.transform(YTest)\n",
    "YTestDummy = np_utils.to_categorical(YTestNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:21:39.536661Z",
     "start_time": "2019-07-01T23:21:33.709701Z"
    }
   },
   "outputs": [],
   "source": [
    "TokenModel = Tokenizer(num_words=5000)\n",
    "TokenModel.fit_on_texts(XTrain.tolist())\n",
    "XTrain = TokenModel.texts_to_sequences(XTrain.tolist())\n",
    "#XTrain = TokenModel.texts_to_matrix(XTrain, mode='tfidf')\n",
    "XTest = TokenModel.texts_to_sequences(XTest.tolist())\n",
    "#XTest = TokenModel.texts_to_matrix(XTest, mode='tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:21:43.452099Z",
     "start_time": "2019-07-01T23:21:42.920357Z"
    }
   },
   "outputs": [],
   "source": [
    "XTrain = pad_sequences(XTrain, padding='post', maxlen=500)\n",
    "XTest = pad_sequences(XTest, padding='post', maxlen=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:21:47.815700Z",
     "start_time": "2019-07-01T23:21:47.733537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_73 (Embedding)     (None, 500, 200)          1000000   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_24 (Glo (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_242 (Dense)            (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "dense_243 (Dense)            (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 1,010,560\n",
      "Trainable params: 1,010,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "InputDim = XTrain.shape[1]\n",
    "Model = Sequential()\n",
    "Model.add(layers.Embedding(input_dim=5000,\n",
    "                           output_dim=200,\n",
    "                           input_length=500))\n",
    "Model.add(layers.GlobalMaxPooling1D())\n",
    "#Model.add(layers.Flatten())\n",
    "Model.add(layers.Dense(50, activation='relu'))\n",
    "#Model.add(layers.SpatialDropout1D(0.2))\n",
    "#Model.add(layers.GRU(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
    "#Model.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "#Model.add(layers.Dense(40, activation='relu'))\n",
    "#Model.add(layers.Dropout(rate = 0.3))\n",
    "Model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "Model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"])\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:26:50.166588Z",
     "start_time": "2019-07-01T23:22:01.668968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16793 samples, validate on 5598 samples\n",
      "Epoch 1/20\n",
      "16793/16793 [==============================] - 22s 1ms/step - loss: 1.3250 - acc: 0.5481 - val_loss: 1.1879 - val_acc: 0.5927\n",
      "Epoch 2/20\n",
      "16793/16793 [==============================] - 14s 834us/step - loss: 1.1117 - acc: 0.6258 - val_loss: 1.1190 - val_acc: 0.6286\n",
      "Epoch 3/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.9899 - acc: 0.6754 - val_loss: 1.0751 - val_acc: 0.6454\n",
      "Epoch 4/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.8639 - acc: 0.7230 - val_loss: 1.0719 - val_acc: 0.6651\n",
      "Epoch 5/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.7539 - acc: 0.7621 - val_loss: 1.0529 - val_acc: 0.6772\n",
      "Epoch 6/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.6506 - acc: 0.7949 - val_loss: 1.0791 - val_acc: 0.6720\n",
      "Epoch 7/20\n",
      "16793/16793 [==============================] - 14s 833us/step - loss: 0.5650 - acc: 0.8183 - val_loss: 1.0992 - val_acc: 0.6920\n",
      "Epoch 8/20\n",
      "16793/16793 [==============================] - 14s 835us/step - loss: 0.4947 - acc: 0.8415 - val_loss: 1.1201 - val_acc: 0.6972\n",
      "Epoch 9/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.4369 - acc: 0.8612 - val_loss: 1.1625 - val_acc: 0.6986\n",
      "Epoch 10/20\n",
      "16793/16793 [==============================] - 14s 835us/step - loss: 0.3936 - acc: 0.8686 - val_loss: 1.2111 - val_acc: 0.6793\n",
      "Epoch 11/20\n",
      "16793/16793 [==============================] - 14s 834us/step - loss: 0.3587 - acc: 0.8805 - val_loss: 1.2259 - val_acc: 0.7053\n",
      "Epoch 12/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.3292 - acc: 0.8883 - val_loss: 1.2536 - val_acc: 0.7019\n",
      "Epoch 13/20\n",
      "16793/16793 [==============================] - 14s 835us/step - loss: 0.3113 - acc: 0.8922 - val_loss: 1.2640 - val_acc: 0.6972\n",
      "Epoch 14/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.2980 - acc: 0.8948 - val_loss: 1.2707 - val_acc: 0.7047\n",
      "Epoch 15/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.2851 - acc: 0.8966 - val_loss: 1.2869 - val_acc: 0.7074\n",
      "Epoch 16/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.2700 - acc: 0.9035 - val_loss: 1.2964 - val_acc: 0.7006\n",
      "Epoch 17/20\n",
      "16793/16793 [==============================] - 14s 834us/step - loss: 0.2670 - acc: 0.9030 - val_loss: 1.3586 - val_acc: 0.7147\n",
      "Epoch 18/20\n",
      "16793/16793 [==============================] - 14s 832us/step - loss: 0.2584 - acc: 0.9028 - val_loss: 1.2986 - val_acc: 0.7153\n",
      "Epoch 19/20\n",
      "16793/16793 [==============================] - 14s 833us/step - loss: 0.2503 - acc: 0.9047 - val_loss: 1.3184 - val_acc: 0.7110\n",
      "Epoch 20/20\n",
      "16793/16793 [==============================] - 14s 833us/step - loss: 0.2478 - acc: 0.9056 - val_loss: 1.2987 - val_acc: 0.7101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1268f8f630>"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(XTrain, \n",
    "          YTrainDummy,\n",
    "          epochs=20,\n",
    "          validation_data=(XTest, YTestDummy),\n",
    "          batch_size=25,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-01T23:31:03.705993Z",
     "start_time": "2019-07-01T23:31:01.033214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9270529387244685\n",
      "0.710075026795284\n",
      "ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "print(f1_score(YTrainNum,Model.predict_classes(XTrain), average=\"micro\"))\n",
    "print(f1_score(YTestNum,Model.predict_classes(XTest), average=\"micro\"))\n",
    "confusion_matrix(YTestNum,Model.predict_classes(XTest))\n",
    "print(\"ðŸ˜Š\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
